{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88a884a-fe87-4676-9979-c8abe3161adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hyarrava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76de2d6-c4ac-49f8-af65-ecf0f6b82380",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85ccc54-40c2-4122-8b8a-8eaa762fbae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/news_summary_more.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a3843c-9384-4b5f-9535-3d06f9c84f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98401</td>\n",
       "      <td>98401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>98280</td>\n",
       "      <td>98360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Warne produced 'ball of century' with his 1st ...</td>\n",
       "      <td>Virender Sehwag was captaining India when he h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headlines  \\\n",
       "count                                               98401   \n",
       "unique                                              98280   \n",
       "top     Warne produced 'ball of century' with his 1st ...   \n",
       "freq                                                    3   \n",
       "\n",
       "                                                     text  \n",
       "count                                               98401  \n",
       "unique                                              98360  \n",
       "top     Virender Sehwag was captaining India when he h...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260384a0-134a-4d47-8729-714d847a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98401 entries, 0 to 98400\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   headlines  98401 non-null  object\n",
      " 1   text       98401 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d874925-b386-4ffb-8168-47dbf038dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb704ef-7131-4e30-bee2-1315323bd88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04475602-ed14-461e-9622-cec3196c0e59",
   "metadata": {},
   "source": [
    "### Punctuation removal, stop word removal, loweing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d7112d-0833-4a41-ac42-3977786b2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d76b160-e60d-4570-ac7c-87c0db1db3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct_stop_words(sentence):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    punct_sentence = sentence.translate(translator)\n",
    "    \n",
    "    clean_words = []\n",
    "    for word in punct_sentence.split(' '):\n",
    "        if word not in stop_words:\n",
    "            clean_words.append(word.lower())\n",
    "\n",
    "    return ' '.join([word for word in clean_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9fb5a50-9cfe-4b0c-9f02-aee4df3939c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"\"\"'Ousted Nissan Chairman Carlos Ghosn has said his arrest over alleged financial misconduct was led by \"plot and treason\" by the Japanese carmaker\\'s executives who opposed its deeper integration with Renault and Mitsubishi. Ghosn added he had discussed the integration plans with Nissan\\'s CEO in September, a month before his arrest. He further said he wouldn\\'t flee if granted bail.'\"\"\"\n",
    "\n",
    "clean_sentence = remove_punct_stop_words(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e526480-58ee-4b91-94ec-cda2acf754a8",
   "metadata": {},
   "source": [
    "### Punctuation removal, stop word removal, loweing \n",
    "### may not be helpful in this case as they cutdown the fluency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48690562-49fe-46ba-a556-71a35f1ef3a8",
   "metadata": {},
   "source": [
    "## We wil be movng without them\n",
    "## We will do tokenization and then Word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba97f8e-92c1-496c-8363-3522051d2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the Summarize text class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Summarize_text(Dataset):\n",
    "    def __init__(self, txt, summary, tokenizer, max_length = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.txt = txt\n",
    "        self.summary = summary\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokens_list, self.attn_masks_list = self.tokenize_words(self.txt)\n",
    "        self.smr_tokens_list, self.smr_attn_masks_list = self.tokenize_words(self.summary)\n",
    "\n",
    "\n",
    "    def tokenize_words(self, text):\n",
    "        tokens_list, attn_masks_list = [], []\n",
    "        for line in text:\n",
    "            tokens = tokenizer(line, padding = 'max_length', truncation = True,\n",
    "                                return_tensors = \"pt\", max_length = self.max_length)\n",
    "            input_ids , attention_masks = tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
    "            tokens_list.append(input_ids.squeeze(0))\n",
    "            attn_masks_list.append(attention_masks.squeeze(0))\n",
    "        return tokens_list,  attn_masks_list\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\" : self.tokens_list[idx],\n",
    "            \"attention_mask\" : self.attn_masks_list[idx],\n",
    "            \"labels\" : self.smr_tokens_list[idx],\n",
    "            \"labels_attention_mask\" : self.smr_attn_masks_list[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97e7c1c-7b19-4895-9a34-9862442736cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyarrava/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader_V1(text, headlines, max_length = 512, batch_size = 4, stride =4, num_workers = 4):\n",
    "    dataset = Summarize_text(text,headlines,tokenizer, max_length = max_length)\n",
    "    dataloader= DataLoader(dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = num_workers\n",
    "                )\n",
    "    return dataloader\n",
    "\n",
    "headlines = data[\"headlines\"].to_list()\n",
    "text = data[\"text\"].to_list()\n",
    "\n",
    "## Tokenization\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"slauw87/bart_summarisation\")\n",
    "dataloader = create_dataloader_V1(text, headlines, max_length = 512, batch_size = 4, stride =4, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aaf1783-ee47-4b0f-a0bb-7cd93be45132",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Each batch of size (4,4,512)\n",
    "### 4 samples in each batch\n",
    "### Each sample consists of 512 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "387e3247-2f2d-4d37-9e9a-33d1560781ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model Architecture\n",
    "### Encoder\n",
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f162a572-79c8-470b-bd79-5c808e6d2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size,emb_dim, hidden_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim \n",
    "\n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx = tokenizer.pad_token_id)\n",
    "        self.lstm_layers = nn.LSTM(self.emb_dim, self.hidden_dim, self.num_layers, \n",
    "                                   dropout= 0.2, bidirectional = False, batch_first = True)\n",
    "        self.dropout_layer = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.embedding_layer(x)\n",
    "        lstm_output, (hidden_state, cell_state) = self.lstm_layers(out)\n",
    "        out = self.dropout_layer(lstm_output)\n",
    "        return out, (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70a9471e-b579-4f23-99f2-97009b5b8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        self.lstm_layer = nn.LSTM(self.emb_dim, self.hidden_dim, num_layers=self.num_layers,\n",
    "                                  bidirectional=False, batch_first=True, dropout=0.1)  # Set bidirectional=False\n",
    "        self.dropout_layer = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)  # Changed to hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        out = self.embedding_layer(x)\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm_layer(out, (hidden_state, cell_state))\n",
    "        out = self.dropout_layer(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden_state, cell_state  # Return hidden and cell states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb8a345b-573e-4141-9a42-f9cdee741353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/250], Loss: 5288.4595\n",
      "Epoch [1/10], Step [20/250], Loss: 4077.8938\n",
      "Epoch [1/10], Step [30/250], Loss: 2872.7795\n",
      "Epoch [1/10], Step [40/250], Loss: 1843.0858\n",
      "Epoch [1/10], Step [50/250], Loss: 1036.4354\n",
      "Epoch [1/10], Step [60/250], Loss: 534.9172\n",
      "Epoch [1/10], Step [70/250], Loss: 320.6650\n",
      "Epoch [1/10], Step [80/250], Loss: 247.9296\n",
      "Epoch [1/10], Step [90/250], Loss: 211.5475\n",
      "Epoch [1/10], Step [100/250], Loss: 242.7027\n",
      "Epoch [1/10], Step [110/250], Loss: 201.6288\n",
      "Epoch [1/10], Step [120/250], Loss: 190.0233\n",
      "Epoch [1/10], Step [130/250], Loss: 167.1587\n",
      "Epoch [1/10], Step [140/250], Loss: 154.6165\n",
      "Epoch [1/10], Step [150/250], Loss: 174.9448\n",
      "Epoch [1/10], Step [160/250], Loss: 162.2901\n",
      "Epoch [1/10], Step [170/250], Loss: 167.7577\n",
      "Epoch [1/10], Step [180/250], Loss: 206.8517\n",
      "Epoch [1/10], Step [190/250], Loss: 175.6917\n",
      "Epoch [1/10], Step [200/250], Loss: 147.1778\n",
      "Epoch [1/10], Step [210/250], Loss: 161.6917\n",
      "Epoch [1/10], Step [220/250], Loss: 185.5316\n",
      "Epoch [1/10], Step [230/250], Loss: 138.6581\n",
      "Epoch [1/10], Step [240/250], Loss: 154.1859\n",
      "Epoch [1/10], Step [250/250], Loss: 148.9063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [10/250], Loss: 136.4793\n",
      "Epoch [2/10], Step [20/250], Loss: 154.1091\n",
      "Epoch [2/10], Step [30/250], Loss: 148.3020\n",
      "Epoch [2/10], Step [40/250], Loss: 142.3313\n",
      "Epoch [2/10], Step [50/250], Loss: 124.2923\n",
      "Epoch [2/10], Step [60/250], Loss: 153.4417\n",
      "Epoch [2/10], Step [70/250], Loss: 134.6255\n",
      "Epoch [2/10], Step [80/250], Loss: 143.9094\n",
      "Epoch [2/10], Step [90/250], Loss: 119.9430\n",
      "Epoch [2/10], Step [100/250], Loss: 142.2036\n",
      "Epoch [2/10], Step [110/250], Loss: 148.8927\n",
      "Epoch [2/10], Step [120/250], Loss: 123.2168\n",
      "Epoch [2/10], Step [130/250], Loss: 128.0288\n",
      "Epoch [2/10], Step [140/250], Loss: 124.3980\n",
      "Epoch [2/10], Step [150/250], Loss: 116.6490\n",
      "Epoch [2/10], Step [160/250], Loss: 133.0189\n",
      "Epoch [2/10], Step [170/250], Loss: 147.9809\n",
      "Epoch [2/10], Step [180/250], Loss: 124.1066\n",
      "Epoch [2/10], Step [190/250], Loss: 123.1701\n",
      "Epoch [2/10], Step [200/250], Loss: 133.4922\n",
      "Epoch [2/10], Step [210/250], Loss: 138.8601\n",
      "Epoch [2/10], Step [220/250], Loss: 118.2033\n",
      "Epoch [2/10], Step [230/250], Loss: 113.6094\n",
      "Epoch [2/10], Step [240/250], Loss: 135.8747\n",
      "Epoch [2/10], Step [250/250], Loss: 138.5711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [10/250], Loss: 137.9628\n",
      "Epoch [3/10], Step [20/250], Loss: 112.1264\n",
      "Epoch [3/10], Step [30/250], Loss: 129.7979\n",
      "Epoch [3/10], Step [40/250], Loss: 127.1412\n",
      "Epoch [3/10], Step [50/250], Loss: 127.3604\n",
      "Epoch [3/10], Step [60/250], Loss: 123.8983\n",
      "Epoch [3/10], Step [70/250], Loss: 116.8362\n",
      "Epoch [3/10], Step [80/250], Loss: 100.2570\n",
      "Epoch [3/10], Step [90/250], Loss: 125.1436\n",
      "Epoch [3/10], Step [100/250], Loss: 120.1727\n",
      "Epoch [3/10], Step [110/250], Loss: 126.6572\n",
      "Epoch [3/10], Step [120/250], Loss: 135.2001\n",
      "Epoch [3/10], Step [130/250], Loss: 130.3113\n",
      "Epoch [3/10], Step [140/250], Loss: 116.8130\n",
      "Epoch [3/10], Step [150/250], Loss: 118.1501\n",
      "Epoch [3/10], Step [160/250], Loss: 137.3941\n",
      "Epoch [3/10], Step [170/250], Loss: 111.2843\n",
      "Epoch [3/10], Step [180/250], Loss: 113.9249\n",
      "Epoch [3/10], Step [190/250], Loss: 108.0497\n",
      "Epoch [3/10], Step [200/250], Loss: 131.1588\n",
      "Epoch [3/10], Step [210/250], Loss: 114.5397\n",
      "Epoch [3/10], Step [220/250], Loss: 112.5743\n",
      "Epoch [3/10], Step [230/250], Loss: 120.6048\n",
      "Epoch [3/10], Step [240/250], Loss: 118.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [250/250], Loss: 122.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [10/250], Loss: 117.2511\n",
      "Epoch [4/10], Step [20/250], Loss: 100.6183\n",
      "Epoch [4/10], Step [30/250], Loss: 127.9222\n",
      "Epoch [4/10], Step [40/250], Loss: 126.8414\n",
      "Epoch [4/10], Step [50/250], Loss: 111.9195\n",
      "Epoch [4/10], Step [60/250], Loss: 127.7744\n",
      "Epoch [4/10], Step [70/250], Loss: 122.5267\n",
      "Epoch [4/10], Step [80/250], Loss: 133.7843\n",
      "Epoch [4/10], Step [90/250], Loss: 124.8258\n",
      "Epoch [4/10], Step [100/250], Loss: 120.7554\n",
      "Epoch [4/10], Step [110/250], Loss: 120.1002\n",
      "Epoch [4/10], Step [120/250], Loss: 113.7559\n",
      "Epoch [4/10], Step [130/250], Loss: 131.4079\n",
      "Epoch [4/10], Step [140/250], Loss: 130.0333\n",
      "Epoch [4/10], Step [150/250], Loss: 106.6927\n",
      "Epoch [4/10], Step [160/250], Loss: 97.5887\n",
      "Epoch [4/10], Step [170/250], Loss: 106.8365\n",
      "Epoch [4/10], Step [180/250], Loss: 93.7810\n",
      "Epoch [4/10], Step [190/250], Loss: 108.3759\n",
      "Epoch [4/10], Step [200/250], Loss: 118.7685\n",
      "Epoch [4/10], Step [210/250], Loss: 123.4406\n",
      "Epoch [4/10], Step [220/250], Loss: 126.4320\n",
      "Epoch [4/10], Step [230/250], Loss: 113.7856\n",
      "Epoch [4/10], Step [240/250], Loss: 110.5033\n",
      "Epoch [4/10], Step [250/250], Loss: 123.3484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [10/250], Loss: 126.0642\n",
      "Epoch [5/10], Step [20/250], Loss: 116.2042\n",
      "Epoch [5/10], Step [30/250], Loss: 118.7453\n",
      "Epoch [5/10], Step [40/250], Loss: 108.4187\n",
      "Epoch [5/10], Step [50/250], Loss: 103.8763\n",
      "Epoch [5/10], Step [60/250], Loss: 114.6579\n",
      "Epoch [5/10], Step [70/250], Loss: 130.2008\n",
      "Epoch [5/10], Step [80/250], Loss: 100.7407\n",
      "Epoch [5/10], Step [90/250], Loss: 121.6217\n",
      "Epoch [5/10], Step [100/250], Loss: 129.5717\n",
      "Epoch [5/10], Step [110/250], Loss: 103.3908\n",
      "Epoch [5/10], Step [120/250], Loss: 125.3384\n",
      "Epoch [5/10], Step [130/250], Loss: 145.4140\n",
      "Epoch [5/10], Step [140/250], Loss: 123.2391\n",
      "Epoch [5/10], Step [150/250], Loss: 111.9265\n",
      "Epoch [5/10], Step [160/250], Loss: 107.1448\n",
      "Epoch [5/10], Step [170/250], Loss: 116.6856\n",
      "Epoch [5/10], Step [180/250], Loss: 103.7813\n",
      "Epoch [5/10], Step [190/250], Loss: 104.6075\n",
      "Epoch [5/10], Step [200/250], Loss: 117.0537\n",
      "Epoch [5/10], Step [210/250], Loss: 109.5249\n",
      "Epoch [5/10], Step [220/250], Loss: 131.6974\n",
      "Epoch [5/10], Step [230/250], Loss: 130.1164\n",
      "Epoch [5/10], Step [240/250], Loss: 117.5854\n",
      "Epoch [5/10], Step [250/250], Loss: 137.2271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [10/250], Loss: 102.9738\n",
      "Epoch [6/10], Step [20/250], Loss: 120.6307\n",
      "Epoch [6/10], Step [30/250], Loss: 103.4153\n",
      "Epoch [6/10], Step [40/250], Loss: 101.6525\n",
      "Epoch [6/10], Step [50/250], Loss: 115.7540\n",
      "Epoch [6/10], Step [60/250], Loss: 116.5428\n",
      "Epoch [6/10], Step [70/250], Loss: 133.5904\n",
      "Epoch [6/10], Step [80/250], Loss: 133.1753\n",
      "Epoch [6/10], Step [90/250], Loss: 112.0092\n",
      "Epoch [6/10], Step [100/250], Loss: 116.1924\n",
      "Epoch [6/10], Step [110/250], Loss: 120.6336\n",
      "Epoch [6/10], Step [120/250], Loss: 108.5417\n",
      "Epoch [6/10], Step [130/250], Loss: 114.5824\n",
      "Epoch [6/10], Step [140/250], Loss: 106.0465\n",
      "Epoch [6/10], Step [150/250], Loss: 107.0128\n",
      "Epoch [6/10], Step [160/250], Loss: 104.5076\n",
      "Epoch [6/10], Step [170/250], Loss: 117.6576\n",
      "Epoch [6/10], Step [180/250], Loss: 123.2336\n",
      "Epoch [6/10], Step [190/250], Loss: 117.2726\n",
      "Epoch [6/10], Step [200/250], Loss: 112.1479\n",
      "Epoch [6/10], Step [210/250], Loss: 126.4739\n",
      "Epoch [6/10], Step [220/250], Loss: 113.6585\n",
      "Epoch [6/10], Step [230/250], Loss: 102.0733\n",
      "Epoch [6/10], Step [240/250], Loss: 112.8563\n",
      "Epoch [6/10], Step [250/250], Loss: 120.9278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [10/250], Loss: 108.1268\n",
      "Epoch [7/10], Step [20/250], Loss: 128.9407\n",
      "Epoch [7/10], Step [30/250], Loss: 106.4290\n",
      "Epoch [7/10], Step [40/250], Loss: 104.4338\n",
      "Epoch [7/10], Step [50/250], Loss: 107.7215\n",
      "Epoch [7/10], Step [60/250], Loss: 111.8962\n",
      "Epoch [7/10], Step [70/250], Loss: 96.1509\n",
      "Epoch [7/10], Step [80/250], Loss: 100.4778\n",
      "Epoch [7/10], Step [90/250], Loss: 117.1059\n",
      "Epoch [7/10], Step [100/250], Loss: 114.1706\n",
      "Epoch [7/10], Step [110/250], Loss: 100.6361\n",
      "Epoch [7/10], Step [120/250], Loss: 109.3726\n",
      "Epoch [7/10], Step [130/250], Loss: 109.1886\n",
      "Epoch [7/10], Step [140/250], Loss: 128.1091\n",
      "Epoch [7/10], Step [150/250], Loss: 107.3138\n",
      "Epoch [7/10], Step [160/250], Loss: 99.7722\n",
      "Epoch [7/10], Step [170/250], Loss: 121.5961\n",
      "Epoch [7/10], Step [180/250], Loss: 111.7435\n",
      "Epoch [7/10], Step [190/250], Loss: 102.3688\n",
      "Epoch [7/10], Step [200/250], Loss: 128.9720\n",
      "Epoch [7/10], Step [210/250], Loss: 114.6114\n",
      "Epoch [7/10], Step [220/250], Loss: 123.8648\n",
      "Epoch [7/10], Step [230/250], Loss: 103.6909\n",
      "Epoch [7/10], Step [240/250], Loss: 121.8152\n",
      "Epoch [7/10], Step [250/250], Loss: 113.8540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [10/250], Loss: 122.3809\n",
      "Epoch [8/10], Step [20/250], Loss: 99.9339\n",
      "Epoch [8/10], Step [30/250], Loss: 89.9272\n",
      "Epoch [8/10], Step [40/250], Loss: 106.7215\n",
      "Epoch [8/10], Step [50/250], Loss: 117.0284\n",
      "Epoch [8/10], Step [60/250], Loss: 110.2725\n",
      "Epoch [8/10], Step [70/250], Loss: 117.2591\n",
      "Epoch [8/10], Step [80/250], Loss: 111.2425\n",
      "Epoch [8/10], Step [90/250], Loss: 100.3124\n",
      "Epoch [8/10], Step [100/250], Loss: 110.5373\n",
      "Epoch [8/10], Step [110/250], Loss: 103.1211\n",
      "Epoch [8/10], Step [120/250], Loss: 99.6969\n",
      "Epoch [8/10], Step [130/250], Loss: 134.3621\n",
      "Epoch [8/10], Step [140/250], Loss: 111.7913\n",
      "Epoch [8/10], Step [150/250], Loss: 115.6362\n",
      "Epoch [8/10], Step [160/250], Loss: 105.3374\n",
      "Epoch [8/10], Step [170/250], Loss: 95.1773\n",
      "Epoch [8/10], Step [180/250], Loss: 100.6977\n",
      "Epoch [8/10], Step [190/250], Loss: 115.4196\n",
      "Epoch [8/10], Step [200/250], Loss: 132.9806\n",
      "Epoch [8/10], Step [210/250], Loss: 119.4558\n",
      "Epoch [8/10], Step [220/250], Loss: 104.8916\n",
      "Epoch [8/10], Step [230/250], Loss: 117.6818\n",
      "Epoch [8/10], Step [240/250], Loss: 106.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [250/250], Loss: 108.3817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [10/250], Loss: 101.1842\n",
      "Epoch [9/10], Step [20/250], Loss: 104.3108\n",
      "Epoch [9/10], Step [30/250], Loss: 119.3657\n",
      "Epoch [9/10], Step [40/250], Loss: 112.9912\n",
      "Epoch [9/10], Step [50/250], Loss: 112.8099\n",
      "Epoch [9/10], Step [60/250], Loss: 81.9102\n",
      "Epoch [9/10], Step [70/250], Loss: 122.0830\n",
      "Epoch [9/10], Step [80/250], Loss: 97.2430\n",
      "Epoch [9/10], Step [90/250], Loss: 105.3666\n",
      "Epoch [9/10], Step [100/250], Loss: 104.0759\n",
      "Epoch [9/10], Step [110/250], Loss: 105.2614\n",
      "Epoch [9/10], Step [120/250], Loss: 123.0393\n",
      "Epoch [9/10], Step [130/250], Loss: 112.6892\n",
      "Epoch [9/10], Step [140/250], Loss: 114.1610\n",
      "Epoch [9/10], Step [150/250], Loss: 86.4679\n",
      "Epoch [9/10], Step [160/250], Loss: 100.5414\n",
      "Epoch [9/10], Step [170/250], Loss: 116.5331\n",
      "Epoch [9/10], Step [180/250], Loss: 104.5601\n",
      "Epoch [9/10], Step [190/250], Loss: 93.5961\n",
      "Epoch [9/10], Step [200/250], Loss: 104.7480\n",
      "Epoch [9/10], Step [210/250], Loss: 107.0865\n",
      "Epoch [9/10], Step [220/250], Loss: 94.1099\n",
      "Epoch [9/10], Step [230/250], Loss: 96.0339\n",
      "Epoch [9/10], Step [240/250], Loss: 124.1254\n",
      "Epoch [9/10], Step [250/250], Loss: 110.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [10/250], Loss: 110.2848\n",
      "Epoch [10/10], Step [20/250], Loss: 92.2484\n",
      "Epoch [10/10], Step [30/250], Loss: 103.7044\n",
      "Epoch [10/10], Step [40/250], Loss: 124.2909\n",
      "Epoch [10/10], Step [50/250], Loss: 113.7615\n",
      "Epoch [10/10], Step [60/250], Loss: 84.6242\n",
      "Epoch [10/10], Step [70/250], Loss: 121.9430\n",
      "Epoch [10/10], Step [80/250], Loss: 106.9101\n",
      "Epoch [10/10], Step [90/250], Loss: 102.3693\n",
      "Epoch [10/10], Step [100/250], Loss: 91.8315\n",
      "Epoch [10/10], Step [110/250], Loss: 97.2724\n",
      "Epoch [10/10], Step [120/250], Loss: 113.0755\n",
      "Epoch [10/10], Step [130/250], Loss: 112.4084\n",
      "Epoch [10/10], Step [140/250], Loss: 106.8098\n",
      "Epoch [10/10], Step [150/250], Loss: 89.9621\n",
      "Epoch [10/10], Step [160/250], Loss: 104.1515\n",
      "Epoch [10/10], Step [170/250], Loss: 100.5593\n",
      "Epoch [10/10], Step [180/250], Loss: 106.0526\n",
      "Epoch [10/10], Step [190/250], Loss: 115.6786\n",
      "Epoch [10/10], Step [200/250], Loss: 93.4203\n",
      "Epoch [10/10], Step [210/250], Loss: 116.5991\n",
      "Epoch [10/10], Step [220/250], Loss: 116.8332\n",
      "Epoch [10/10], Step [230/250], Loss: 97.5359\n",
      "Epoch [10/10], Step [240/250], Loss: 111.7455\n",
      "Epoch [10/10], Step [250/250], Loss: 113.9669\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming necessary imports and data preparation are done\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "vocab_size = tokenizer.vocab_size\n",
    "batch_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(vocab_size=vocab_size, emb_dim=512, hidden_dim=64, num_layers=4).to(device)\n",
    "decoder = Decoder(vocab_size=vocab_size, emb_dim=512, hidden_dim=64, num_layers=4).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Encode the input\n",
    "        encoder_out, (encoder_hidden, encoder_cell) = encoder(input_ids)\n",
    "\n",
    "        # Initialize decoder input (start with <BOS> token)\n",
    "        decoder_input = labels[:, 0].unsqueeze(1)  # Start with the first token of the labels\n",
    "        decoder_hidden = encoder_hidden  # Initialize with encoder hidden state\n",
    "        decoder_cell = encoder_cell  # Initialize with encoder cell state\n",
    "\n",
    "        # Initialize tensor for decoder outputs\n",
    "        decoder_outputs = torch.zeros(labels.size(0), labels.size(1) - 1, vocab_size).to(device)\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(1, labels.size(1)):  # Start from the second token\n",
    "            output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "\n",
    "            # Store the output\n",
    "            decoder_outputs[:, t - 1, :] = output.squeeze(1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss += criterion(output.view(-1, vocab_size), labels[:, t].view(-1))\n",
    "\n",
    "            # Prepare next input for the decoder (teacher forcing)\n",
    "            decoder_input = labels[:, t].unsqueeze(1)  # Use true label for next input\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "}, 'encoder_decoder_model.pth')\n",
    "\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6bf982c-bd61-4a21-a2ba-499afd8058d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(input_text, tokenizer):\n",
    "    # Tokenize and convert to tensor\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aafef744-489e-4d69-8b98-3b037b53056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input_ids, encoder, device):\n",
    "    input_ids = input_ids.to(device)\n",
    "    encoder_out, encoder_hidden = encoder(input_ids)\n",
    "    return encoder_out, encoder_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20285c66-e801-4b0d-a67e-2c22e213ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(input_ids, encoder, decoder, device, max_length=50):\n",
    "    encoder_out, (encoder_hidden, encoder_cell) = encode_input(input_ids, encoder, device)\n",
    "\n",
    "    # Start decoding\n",
    "    decoder_input = torch.tensor([[tokenizer.bos_token_id]]).to(device)  # BOS token\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    summary = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "        next_token = output.argmax(2)  # Get the index of the highest probability token\n",
    "        summary.append(next_token.item())\n",
    "\n",
    "        # Stop if the EOS token is generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        decoder_input = next_token  # Use the predicted token as next input\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52ab57cf-6f49-45d8-b2a7-7a3f99e21ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_summary(summary, tokenizer):\n",
    "    return tokenizer.decode(summary, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "546eb1ed-7f6a-4a86-84d7-678499fb872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_summary(input_text, encoder, decoder, tokenizer, device):\n",
    "    input_ids = tokenize_input(input_text, tokenizer)\n",
    "    summary_ids = generate_summary(input_ids, encoder, decoder, device)\n",
    "    summary = decode_summary(summary_ids, tokenizer)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44d3e82c-95ef-4710-a730-f020fa0e8a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: I-- to to to to:::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4182299/2854891248.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('encoder_decoder_model.pth')\n"
     ]
    }
   ],
   "source": [
    "# Load your trained models\n",
    "checkpoint = torch.load('encoder_decoder_model.pth')\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "# Set to evaluation mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Input text to summarize\n",
    "input_text = \"\"\"Then, is there any method to get the total number of iteration for the \"for loop\"?\n",
    "\n",
    "In my NLP problem, the total number of iteration is different from int(n_train_samples/batch_size)...\n",
    "\n",
    "For example, if I truncate train data only 10,000 samples and set the batch size as 1024, then 363 iteration occurs in my NLP problem.\n",
    "\n",
    "I wonder how to get the number of total iteration in \"the for-loop\".\"\"\"\n",
    "\n",
    "# Get the summary\n",
    "summary = predict_summary(input_text, encoder, decoder, tokenizer, device)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ed37fac-0567-412c-b5fa-627045483406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68b6fd-8aca-498a-ac3d-80858d01ce57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
